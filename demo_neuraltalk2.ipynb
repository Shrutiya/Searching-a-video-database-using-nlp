{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo_neuraltalk2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc8c40bf94424f43b7c1957d90ef5b7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_f6ff27569de24d1a9e32a0dd93706d37",
            "_dom_classes": [],
            "description": "Play Next",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_ad3402437e844333a1e03d68860389d1",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          },
          "model_module_version": "1.5.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTkYqFqFNhN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9963d03-2049-413f-9eb7-f2f46250e5b6"
      },
      "source": [
        "#Enabling Tensorflow using TPU engine\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 1.15.2\n",
            "Running on TPU  ['10.98.133.42:8470']\n",
            "WARNING:tensorflow:TPU system %s has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
            "INFO:tensorflow:Initializing the TPU system: 10.98.133.42:8470\n",
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.98.133.42:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 17187712677345604422)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 4319665852127281055)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 3214836817000303880)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15235130718227971614)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10414114149522855860)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5703004808311740559)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 1019978783736517648)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15549401506213541557)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5694040063375248867)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 6805316632350311700)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7152458560255960497)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ggJwTNyzxBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9be1223-a438-4433-87de-72e18734f44c"
      },
      "source": [
        "#importing necessary modules\n",
        "\n",
        "print(tf.__version__)\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/1?tf-hub-format=compressed\"\n",
        "\n",
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.Module(module_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKRJh8RdrlbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e43c20-502f-4295-afc9-2843a6aefb06"
      },
      "source": [
        "#Installing necessary packages\n",
        "\n",
        "!pip install ffmpeg-python\n",
        "!pip3 install --upgrade speechrecognition"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Collecting speechrecognition\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 133kB/s \n",
            "\u001b[?25hInstalling collected packages: speechrecognition\n",
            "Successfully installed speechrecognition-3.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxqJc6wRRQJq"
      },
      "source": [
        "#Speech Recognition Code to enable microphone\n",
        "\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };            \n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "  \n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iDqHq5ccogx"
      },
      "source": [
        "#Function to find semantic similarity of two sentences using Google's Universal Sentence Encoder\n",
        "\n",
        "def semantic_similarity(cap1,cap2):\n",
        "    messages=[cap1,cap2]\n",
        "    similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
        "    similarity_message_encodings = embed(similarity_input_placeholder)\n",
        "    with tf.Session() as session:\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        session.run(tf.tables_initializer())\n",
        "        message_embeddings_ = session.run(similarity_message_encodings, feed_dict={similarity_input_placeholder: messages})    \n",
        "    return cosine_similarity(message_embeddings_[0].reshape(1,-1),message_embeddings_[1].reshape(1,-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsqNAq8eRhc_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "85381f46-7650-4da2-e439-a302628de13a"
      },
      "source": [
        "#Speech to Text conversion to obtain the voice query as text\n",
        "\n",
        "text=0\n",
        "import speech_recognition\n",
        "r = speech_recognition.Recognizer()\n",
        "print(\"Speak Anything :\")\n",
        "audio, sr = get_audio()\n",
        "#print(type(audio))\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "\n",
        "write('test.wav', 44100, audio)\n",
        "with speech_recognition.AudioFile('test.wav') as source:\n",
        "        audio = r.record(source)  # read the entire audio file                  \n",
        "        try:\n",
        "            text = r.recognize_google(audio)\n",
        "            print(\"You said : {}\".format(text))\n",
        "        except:\n",
        "            print(\"Sorry could not recognize what you said\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Speak Anything :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<script>\n",
              "var my_div = document.createElement(\"DIV\");\n",
              "var my_p = document.createElement(\"P\");\n",
              "var my_btn = document.createElement(\"BUTTON\");\n",
              "var t = document.createTextNode(\"Press to start recording\");\n",
              "\n",
              "my_btn.appendChild(t);\n",
              "//my_p.appendChild(my_btn);\n",
              "my_div.appendChild(my_btn);\n",
              "document.body.appendChild(my_div);\n",
              "\n",
              "var base64data = 0;\n",
              "var reader;\n",
              "var recorder, gumStream;\n",
              "var recordButton = my_btn;\n",
              "\n",
              "var handleSuccess = function(stream) {\n",
              "  gumStream = stream;\n",
              "  var options = {\n",
              "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
              "    mimeType : 'audio/webm;codecs=opus'\n",
              "    //mimeType : 'audio/webm;codecs=pcm'\n",
              "  };            \n",
              "  //recorder = new MediaRecorder(stream, options);\n",
              "  recorder = new MediaRecorder(stream);\n",
              "  recorder.ondataavailable = function(e) {            \n",
              "    var url = URL.createObjectURL(e.data);\n",
              "    var preview = document.createElement('audio');\n",
              "    preview.controls = true;\n",
              "    preview.src = url;\n",
              "    document.body.appendChild(preview);\n",
              "\n",
              "    reader = new FileReader();\n",
              "    reader.readAsDataURL(e.data); \n",
              "    reader.onloadend = function() {\n",
              "      base64data = reader.result;\n",
              "      //console.log(\"Inside FileReader:\" + base64data);\n",
              "    }\n",
              "  };\n",
              "  recorder.start();\n",
              "  };\n",
              "\n",
              "recordButton.innerText = \"Recording... press to stop\";\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
              "\n",
              "\n",
              "function toggleRecording() {\n",
              "  if (recorder && recorder.state == \"recording\") {\n",
              "      recorder.stop();\n",
              "      gumStream.getAudioTracks()[0].stop();\n",
              "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
              "  }\n",
              "}\n",
              "\n",
              "// https://stackoverflow.com/a/951057\n",
              "function sleep(ms) {\n",
              "  return new Promise(resolve => setTimeout(resolve, ms));\n",
              "}\n",
              "\n",
              "var data = new Promise(resolve=>{\n",
              "//recordButton.addEventListener(\"click\", toggleRecording);\n",
              "recordButton.onclick = ()=>{\n",
              "toggleRecording()\n",
              "\n",
              "sleep(2000).then(() => {\n",
              "  // wait 2000ms for the data to be available...\n",
              "  // ideally this should use something like await...\n",
              "  //console.log(\"Inside data:\" + base64data)\n",
              "  resolve(base64data.toString())\n",
              "\n",
              "});\n",
              "\n",
              "}\n",
              "});\n",
              "      \n",
              "</script>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "You said : woman is skiing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80d4huqdYVFL"
      },
      "source": [
        "#Final set of tracks for the input video\n",
        "\n",
        "final=[['a person holding a pair of scissors on a table',\n",
        "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]],\n",
        " ['a woman holding a wii remote in her hand', [19, 20, 21, 22]],\n",
        " ['a person holding a pair of scissors in front of their face',\n",
        "  [23, 24, 25, 26, 27, 28, 30, 32, 33, 34, 36, 38]],\n",
        " ['a view of a mountain range from a plane',\n",
        "  [29, 30, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44]],\n",
        " ['a busy city street filled with lots of traffic', [31, 32, 33, 34, 36]],\n",
        " ['a bus is parked on the side of the street', [45, 46, 48, 49]],\n",
        " ['a sign that says UNK UNK on the side of it', [50, 51, 52]],\n",
        " ['a view of a city street with a traffic light on it',\n",
        "  [53,\n",
        "   54,\n",
        "   55,\n",
        "   56,\n",
        "   57,\n",
        "   58,\n",
        "   59,\n",
        "   60,\n",
        "   61,\n",
        "   62,\n",
        "   63,\n",
        "   64,\n",
        "   65,\n",
        "   66,\n",
        "   67,\n",
        "   68,\n",
        "   69,\n",
        "   70,\n",
        "   71,\n",
        "   72,\n",
        "   73,\n",
        "   74,\n",
        "   76,\n",
        "   77,\n",
        "   78,\n",
        "   79,\n",
        "   80,\n",
        "   81,\n",
        "   82,\n",
        "   83,\n",
        "   84,\n",
        "   85,\n",
        "   86,\n",
        "   88,\n",
        "   89,\n",
        "   90,\n",
        "   91,\n",
        "   92,\n",
        "   93,\n",
        "   94,\n",
        "   95,\n",
        "   96,\n",
        "   97,\n",
        "   98,\n",
        "   99,\n",
        "   100,\n",
        "   101,\n",
        "   102,\n",
        "   103,\n",
        "   104,\n",
        "   105,\n",
        "   106]],\n",
        " ['a little girl holding a cat in her hand',\n",
        "  [107,\n",
        "   108,\n",
        "   109,\n",
        "   110,\n",
        "   111,\n",
        "   112,\n",
        "   113,\n",
        "   114,\n",
        "   115,\n",
        "   116,\n",
        "   117,\n",
        "   118,\n",
        "   119,\n",
        "   120,\n",
        "   121,\n",
        "   122,\n",
        "   123,\n",
        "   124,\n",
        "   125,\n",
        "   126,\n",
        "   127,\n",
        "   128,\n",
        "   129,\n",
        "   130]],\n",
        " ['a stop sign is on a pole on a street',\n",
        "  [131,\n",
        "   132,\n",
        "   133,\n",
        "   134,\n",
        "   135,\n",
        "   136,\n",
        "   137,\n",
        "   138,\n",
        "   139,\n",
        "   140,\n",
        "   141,\n",
        "   142,\n",
        "   143,\n",
        "   144,\n",
        "   145,\n",
        "   146,\n",
        "   147,\n",
        "   149]],\n",
        " ['a picture of a plane flying in the sky',\n",
        "  [148, 149, 150, 151, 152, 153, 154]],\n",
        " ['a large building with a large clock tower',\n",
        "  [155,\n",
        "   156,\n",
        "   157,\n",
        "   158,\n",
        "   159,\n",
        "   160,\n",
        "   161,\n",
        "   162,\n",
        "   163,\n",
        "   164,\n",
        "   165,\n",
        "   166,\n",
        "   167,\n",
        "   168,\n",
        "   169,\n",
        "   171,\n",
        "   172,\n",
        "   173,\n",
        "   174,\n",
        "   175,\n",
        "   176,\n",
        "   177,\n",
        "   178,\n",
        "   179,\n",
        "   180,\n",
        "   181,\n",
        "   182,\n",
        "   183,\n",
        "   184,\n",
        "   185,\n",
        "   186,\n",
        "   187,\n",
        "   188,\n",
        "   189,\n",
        "   190,\n",
        "   191,\n",
        "   192,\n",
        "   193,\n",
        "   194,\n",
        "   195,\n",
        "   196,\n",
        "   197,\n",
        "   198,\n",
        "   199,\n",
        "   200,\n",
        "   201,\n",
        "   202,\n",
        "   203]],\n",
        " ['a person holding a cell phone in their hand', [205, 207, 209, 211]],\n",
        " ['a woman is holding a toothbrush in her mouth',\n",
        "  [208, 209, 210, 212, 213, 214, 215, 216, 217, 218, 219, 220, 222, 223, 224]],\n",
        " ['a man and woman are holding a cake', [221, 222, 223, 224]],\n",
        " ['a street sign on a pole on a city street',\n",
        "  [225,\n",
        "   226,\n",
        "   227,\n",
        "   228,\n",
        "   229,\n",
        "   230,\n",
        "   231,\n",
        "   232,\n",
        "   233,\n",
        "   234,\n",
        "   235,\n",
        "   236,\n",
        "   237,\n",
        "   238,\n",
        "   239,\n",
        "   240,\n",
        "   241,\n",
        "   242,\n",
        "   243,\n",
        "   244,\n",
        "   245,\n",
        "   246]],\n",
        " ['a man riding skis down a snow covered slope',\n",
        "  [247,\n",
        "   248,\n",
        "   249,\n",
        "   250,\n",
        "   251,\n",
        "   252,\n",
        "   253,\n",
        "   254,\n",
        "   255,\n",
        "   256,\n",
        "   257,\n",
        "   258,\n",
        "   259,\n",
        "   260,\n",
        "   261,\n",
        "   262,\n",
        "   263,\n",
        "   264,\n",
        "   265,\n",
        "   266,\n",
        "   267,\n",
        "   268,\n",
        "   269,\n",
        "   270,\n",
        "   271,\n",
        "   272,\n",
        "   273,\n",
        "   274,\n",
        "   275]],\n",
        " ['a person standing on a snow covered ski slope',\n",
        "  [280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291]],\n",
        " ['a close up of a person in a mirror', [292, 294, 296, 298, 299, 300, 301]],\n",
        " ['a woman is talking on a cell phone', [295, 296, 298, 299, 300, 301]],\n",
        " ['a man is holding a hot dog in his hand',\n",
        "  [297, 298, 299, 300, 301, 302, 303, 304, 305, 306]],\n",
        " ['a person standing in a room with luggage', [308, 309, 310]],\n",
        " ['a man holding a cake with a candle on it',\n",
        "  [311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 323, 324, 325, 326]],\n",
        " ['a man is eating a sandwich in a restaurant',\n",
        "  [318,\n",
        "   319,\n",
        "   320,\n",
        "   321,\n",
        "   322,\n",
        "   323,\n",
        "   324,\n",
        "   325,\n",
        "   326,\n",
        "   327,\n",
        "   328,\n",
        "   329,\n",
        "   330,\n",
        "   331,\n",
        "   333,\n",
        "   334]],\n",
        " ['a woman sitting at a table with a glass of wine',\n",
        "  [332,\n",
        "   333,\n",
        "   334,\n",
        "   335,\n",
        "   336,\n",
        "   337,\n",
        "   338,\n",
        "   339,\n",
        "   340,\n",
        "   341,\n",
        "   341,\n",
        "   341,\n",
        "   342,\n",
        "   343,\n",
        "   344,\n",
        "   345,\n",
        "   346,\n",
        "   347,\n",
        "   348,\n",
        "   349,\n",
        "   350,\n",
        "   351,\n",
        "   352]],\n",
        " ['a man and a woman eating a hot dog', [353, 354, 355]],\n",
        " ['a woman is taking a picture of herself in a mirror', [360, 362, 364]],\n",
        " ['a woman is holding a baby in her hand',\n",
        "  [356, 357, 359, 361, 363, 364, 365, 366, 368, 369]],\n",
        " ['a traffic light with a red light on it',\n",
        "  [371, 372, 374, 375, 376, 377, 378, 379]],\n",
        " ['a person is standing on a street corner',\n",
        "  [373, 374, 375, 376, 377, 378, 379]],\n",
        " ['a bottle of wine sitting on top of a counter',\n",
        "  [380,\n",
        "   381,\n",
        "   382,\n",
        "   383,\n",
        "   384,\n",
        "   385,\n",
        "   386,\n",
        "   387,\n",
        "   388,\n",
        "   389,\n",
        "   390,\n",
        "   391,\n",
        "   392,\n",
        "   393,\n",
        "   394,\n",
        "   395,\n",
        "   396,\n",
        "   397,\n",
        "   399,\n",
        "   401,\n",
        "   402,\n",
        "   403,\n",
        "   404,\n",
        "   405,\n",
        "   406,\n",
        "   407,\n",
        "   408]],\n",
        " ['a man is eating a sandwich on a plate',\n",
        "  [398, 399, 400, 401, 403, 405, 406, 407, 408, 410, 411, 412, 414]],\n",
        " ['a group of people standing around a table with a laptop', [424, 425, 426]],\n",
        " ['a man is standing in front of a car',\n",
        "  [413,\n",
        "   414,\n",
        "   415,\n",
        "   416,\n",
        "   417,\n",
        "   418,\n",
        "   419,\n",
        "   420,\n",
        "   421,\n",
        "   422,\n",
        "   423,\n",
        "   425,\n",
        "   426,\n",
        "   427,\n",
        "   428,\n",
        "   429,\n",
        "   430,\n",
        "   431,\n",
        "   432,\n",
        "   433,\n",
        "   434,\n",
        "   435,\n",
        "   436,\n",
        "   437,\n",
        "   438]],\n",
        " ['a woman is holding a glass of wine', [454, 455, 456, 457]],\n",
        " ['a close up of a person holding a pair of scissors',\n",
        "  [440,\n",
        "   441,\n",
        "   443,\n",
        "   444,\n",
        "   445,\n",
        "   446,\n",
        "   447,\n",
        "   448,\n",
        "   449,\n",
        "   450,\n",
        "   451,\n",
        "   452,\n",
        "   453,\n",
        "   455,\n",
        "   457,\n",
        "   459,\n",
        "   460]],\n",
        " ['a woman is holding a glass of wine', [461, 462, 463, 464, 465, 466]],\n",
        " ['a man is holding a cell phone in his hand',\n",
        "  [469, 471, 472, 473, 474, 475, 477]],\n",
        " ['a person is holding a black and white photo', [480, 481, 482]],\n",
        " ['a view of a mountain range from the ocean', [483, 484, 485]],\n",
        " ['a man in a suit and tie standing in front of a mirror', [488, 490, 491]],\n",
        " ['a person is standing in front of a window', [486, 487, 489, 490, 492]],\n",
        " ['a person is holding a teddy bear in the dark', [493, 494, 495]],\n",
        " ['a close up of a person holding a pair of scissors',\n",
        "  [496, 497, 498, 499, 501]],\n",
        " ['a traffic light with a red light on it', [502, 503, 504]],\n",
        " ['a man holding a cell phone in his hand', [506, 507, 508]],\n",
        " ['a sign that says UNK UNK UNK on it', [512, 513, 514, 515, 516, 517]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkeZaavUYVV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69808d8-564d-4210-cf73-962acc7a3e19"
      },
      "source": [
        "#Generating tracks and their duration\n",
        "\n",
        "video_length=52 #seconds\n",
        "num_frames=520\n",
        "frame_len=video_length/num_frames\n",
        "print(frame_len)\n",
        "\n",
        "start_end=[]\n",
        "for track in final:\n",
        "    start=int(track[1][0])\n",
        "    end=int(track[1][-1])\n",
        "    #start_end.append([start,end])\n",
        "    start_end.append([track[0],[round(start*frame_len,2),round(end*frame_len,2)]])\n",
        "start_end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['a person holding a pair of scissors on a table', [0.0, 1.8]],\n",
              " ['a woman holding a wii remote in her hand', [1.9, 2.2]],\n",
              " ['a person holding a pair of scissors in front of their face', [2.3, 3.8]],\n",
              " ['a view of a mountain range from a plane', [2.9, 4.4]],\n",
              " ['a busy city street filled with lots of traffic', [3.1, 3.6]],\n",
              " ['a bus is parked on the side of the street', [4.5, 4.9]],\n",
              " ['a sign that says UNK UNK on the side of it', [5.0, 5.2]],\n",
              " ['a view of a city street with a traffic light on it', [5.3, 10.6]],\n",
              " ['a little girl holding a cat in her hand', [10.7, 13.0]],\n",
              " ['a stop sign is on a pole on a street', [13.1, 14.9]],\n",
              " ['a picture of a plane flying in the sky', [14.8, 15.4]],\n",
              " ['a large building with a large clock tower', [15.5, 20.3]],\n",
              " ['a person holding a cell phone in their hand', [20.5, 21.1]],\n",
              " ['a woman is holding a toothbrush in her mouth', [20.8, 22.4]],\n",
              " ['a man and woman are holding a cake', [22.1, 22.4]],\n",
              " ['a street sign on a pole on a city street', [22.5, 24.6]],\n",
              " ['a man riding skis down a snow covered slope', [24.7, 27.5]],\n",
              " ['a person standing on a snow covered ski slope', [28.0, 29.1]],\n",
              " ['a close up of a person in a mirror', [29.2, 30.1]],\n",
              " ['a woman is talking on a cell phone', [29.5, 30.1]],\n",
              " ['a man is holding a hot dog in his hand', [29.7, 30.6]],\n",
              " ['a person standing in a room with luggage', [30.8, 31.0]],\n",
              " ['a man holding a cake with a candle on it', [31.1, 32.6]],\n",
              " ['a man is eating a sandwich in a restaurant', [31.8, 33.4]],\n",
              " ['a woman sitting at a table with a glass of wine', [33.2, 35.2]],\n",
              " ['a man and a woman eating a hot dog', [35.3, 35.5]],\n",
              " ['a woman is taking a picture of herself in a mirror', [36.0, 36.4]],\n",
              " ['a woman is holding a baby in her hand', [35.6, 36.9]],\n",
              " ['a traffic light with a red light on it', [37.1, 37.9]],\n",
              " ['a person is standing on a street corner', [37.3, 37.9]],\n",
              " ['a bottle of wine sitting on top of a counter', [38.0, 40.8]],\n",
              " ['a man is eating a sandwich on a plate', [39.8, 41.4]],\n",
              " ['a group of people standing around a table with a laptop', [42.4, 42.6]],\n",
              " ['a man is standing in front of a car', [41.3, 43.8]],\n",
              " ['a woman is holding a glass of wine', [45.4, 45.7]],\n",
              " ['a close up of a person holding a pair of scissors', [44.0, 46.0]],\n",
              " ['a woman is holding a glass of wine', [46.1, 46.6]],\n",
              " ['a man is holding a cell phone in his hand', [46.9, 47.7]],\n",
              " ['a person is holding a black and white photo', [48.0, 48.2]],\n",
              " ['a view of a mountain range from the ocean', [48.3, 48.5]],\n",
              " ['a man in a suit and tie standing in front of a mirror', [48.8, 49.1]],\n",
              " ['a person is standing in front of a window', [48.6, 49.2]],\n",
              " ['a person is holding a teddy bear in the dark', [49.3, 49.5]],\n",
              " ['a close up of a person holding a pair of scissors', [49.6, 50.1]],\n",
              " ['a traffic light with a red light on it', [50.2, 50.4]],\n",
              " ['a man holding a cell phone in his hand', [50.6, 50.8]],\n",
              " ['a sign that says UNK UNK UNK on it', [51.2, 51.7]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNsQAjOGWFZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c233ca-b3cd-4cbe-b82e-32210489938e"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "\n",
        "c=0\n",
        "for i in start_end:\n",
        "  print(i[0])\n",
        "  #Semantic similarity checked between the text and each entry in the tracklist and video segment retrieved\n",
        "  if(semantic_similarity(i[0],text)>=0.5):\n",
        "    #Padding performed if required\n",
        "    print(\"match\")\n",
        "    if((i[1][0]-0.5)>0 and (i[1][1]-i[1][0])<1.5 and (i[1][1]+0.5)<52):\n",
        "      ffmpeg_extract_subclip(\"RUSSIA_ 1 MINUTE TRAVEL VLOG.mp4\", i[1][0]-0.5, i[1][1]+0.5, targetname=\"video\"+str(c)+\".mp4\")\n",
        "    elif((i[1][0]-0.5)<0 and (i[1][1]-i[1][0])<1.5 and (i[1][1]+0.5)<52):\n",
        "      ffmpeg_extract_subclip(\"RUSSIA_ 1 MINUTE TRAVEL VLOG.mp4\", i[1][0], i[1][1]+1, targetname=\"video\"+str(c)+\".mp4\")   \n",
        "    elif((i[1][0]-0.5)>0 and (i[1][1]-i[1][0])<1.5 and (i[1][1]+0.5)>52):\n",
        "      ffmpeg_extract_subclip(\"RUSSIA_ 1 MINUTE TRAVEL VLOG.mp4\", i[1][0]-1, i[1][1], targetname=\"video\"+str(c)+\".mp4\")\n",
        "    else:\n",
        "      ffmpeg_extract_subclip(\"RUSSIA_ 1 MINUTE TRAVEL VLOG.mp4\", i[1][0], i[1][1], targetname=\"video\"+str(c)+\".mp4\")\n",
        "    #play(c)\n",
        "    c+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a person holding a pair of scissors on a table\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a woman holding a wii remote in her hand\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a person holding a pair of scissors in front of their face\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a view of a mountain range from a plane\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a busy city street filled with lots of traffic\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a bus is parked on the side of the street\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a sign that says UNK UNK on the side of it\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a view of a city street with a traffic light on it\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a little girl holding a cat in her hand\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a stop sign is on a pole on a street\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a picture of a plane flying in the sky\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a large building with a large clock tower\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a person holding a cell phone in their hand\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a woman is holding a toothbrush in her mouth\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmsUk5Zq0xE0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "dc8c40bf94424f43b7c1957d90ef5b7e",
            "f6ff27569de24d1a9e32a0dd93706d37",
            "ad3402437e844333a1e03d68860389d1",
            "cf4f226902504525a962f2bd44aa229a"
          ]
        },
        "outputId": "6d66aab2-b2e4-414e-fa74-951053934428"
      },
      "source": [
        "#Playing of results\n",
        "c=3\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display,clear_output\n",
        "from IPython.display import YouTubeVideo,HTML\n",
        "import base64\n",
        "import io\n",
        "button = widgets.Button(description=\"Play Next\")\n",
        "output = widgets.Output()\n",
        "x=0\n",
        "def on_button_clicked(b):\n",
        "  # Display the message within the output widget.\n",
        "  with output:\n",
        "    clear_output()\n",
        "    global x\n",
        "    video = io.open('video'+str(x)+'.mp4', 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    display(HTML(data='''<video width=\"320\" height=\"240\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''.format(encoded.decode('ascii'))))\n",
        "    print(\"Result \",x+1)\n",
        "    x+=1\n",
        "    x%=c\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "display(button, output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc8c40bf94424f43b7c1957d90ef5b7e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Button(description='Play Next', style=ButtonStyle())"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf4f226902504525a962f2bd44aa229a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epBgtMw20N9w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}